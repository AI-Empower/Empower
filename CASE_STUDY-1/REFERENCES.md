# References in Different Citation Formats
This document provides a consolidated list of references used in our study, formatted in three popular citation styles: Harvard, MLA, and APA. We've organized them alphabetically by the last name of the first author for your convenience. 
Whether you're cross-referencing, citing, or simply reviewing the foundational works for our study, this page will be a handy resource. Happy researching!

## Harvard

| Author(s) | Year | Title | Source | URL/DOI |
|-----------|------|-------|--------|---------|
| Khot, T. et al. | 2023 | Decomposed Prompting: A Modular Approach for Solving Complex Tasks | arXiv.org | [https://doi.org/10.48550/arXiv.2210.02406](https://doi.org/10.48550/arXiv.2210.02406) |
| Kojima, T. et al. | 2022 | Large Language Models are Zero-Shot Reasoners | arXiv.org | [https://arxiv.org/abs/2205.11916](https://arxiv.org/abs/2205.11916) |
| Wei, J. et al. | 2023 | Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Chain-of-Thought Prompting | arXiv.org | [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903) |

## MLA

| Author(s) | Title | Source | Year | URL/DOI |
|-----------|-------|--------|------|---------|
| Khot, T. et al. | "Decomposed Prompting: A Modular Approach for Solving Complex Tasks" | arXiv.org | 2023 | [https://doi.org/10.48550/arXiv.2210.02406](https://doi.org/10.48550/arXiv.2210.02406) |
| Kojima, T. et al. | "Large Language Models are Zero-Shot Reasoners" | arXiv.org | 2022 | [https://arxiv.org/abs/2205.11916](https://arxiv.org/abs/2205.11916) |
| Wei, J. et al. | "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Chain-of-Thought Prompting" | arXiv.org | 2023 | [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903) |

## APA

| Author(s) | Year | Title | Source | URL/DOI |
|-----------|------|-------|--------|---------|
| Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P. & Sabharwal, A. | 2023 | Decomposed Prompting: A Modular Approach for Solving Complex Tasks | arXiv.org | [https://doi.org/10.48550/arXiv.2210.02406](https://doi.org/10.48550/arXiv.2210.02406) |
| Kojima, T., Gu, S.S., Reid, M., Matsuo, Y. & Iwasawa, Y. | 2022 | Large Language Models are Zero-Shot Reasoners | arXiv.org | [https://arxiv.org/abs/2205.11916](https://arxiv.org/abs/2205.11916) |
| Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi Quoc, E., Le, V. & Zhou, D. | 2023 | Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Chain-of-Thought Prompting | arXiv.org | [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903) |
