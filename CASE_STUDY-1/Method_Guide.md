## Introduction

This guide offers the comprehensive methodology for a comparative case study that evaluates three distinct prompting techniques using standardized reasoning tasks and Large Language Models (LLMs) available through free tiers. The study addresses the existing gaps in the systematic benchmarking of prompting techniques, aiming to compare performance across different models and extract optimization insights through standardized tests.

## Research Design 

The research employs a quantitative experimental approach, comparing performance metrics across various techniques and models. This quantitative analysis is complemented by a qualitative assessment, providing a holistic view of the techniques' effectiveness.

## Data Collection

Data for this study will be sourced from the following datasets:

- **Arithmetic reasoning:** The GSM8K dataset. The specific API or interface for data retrieval is yet to be determined.
- **Implicit reasoning:** Data will be sourced from the StrategyQA dataset via an unspecified API or interface.
- **Commonsense reasoning:** The study will utilize the CSQA dataset, accessed through an as-yet-undetermined API or interface.

## Data Analysis 

The data analysis will encompass both quantitative and qualitative methods. Quantitative metrics, such as accuracy, will be supplemented by human judgments assessing coherence and relevance. The analysis will highlight the differences in optimization between various techniques and models. It's crucial to remain vigilant of potential biases and consider the possibility of crowd-sourcing some evaluations for a more diverse perspective.

## Validity and Reliability

- **Prompt Design:** The design of the prompts will be validated through feedback from peers in the field.
- **Documentation:** All protocols will be meticulously documented to ensure reproducibility.
- **Sampling:** Multiple samples will be taken for each condition to ensure a comprehensive analysis.

## Limitations

- **Sample Size:** The study acknowledges the limitation of a small sample size, which might affect the generalizability of the findings.
- **Application Gaps:** There might be gaps when translating the findings to real-world applications.
- **Prompt Design:** There's a potential for flaws in the prompt design, which might affect the results.

## Conclusion

This methodology guide offers a detailed overview of the comparative experimental approach used to evaluate prompting techniques on reasoning tasks. The study uses standardized datasets and LLMs that are freely accessible. The findings from this study aim to provide actionable insights for the field of prompt architecture.
